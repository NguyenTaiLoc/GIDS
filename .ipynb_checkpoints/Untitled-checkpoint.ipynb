{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b619be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense \n",
    "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#loading CAN IDs from file\n",
    "fp = open('/home/tailoc/Downloads/GIDS_dataset/CAN_IDS/test_data.txt', 'r')\n",
    "content = fp.readlines()\n",
    "\n",
    "data = []\n",
    "for i in range(len(content[0])):\n",
    "\tif (content[0][i] != ','):\n",
    "\t\tdata.append(content[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "324a27bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding data\n",
    "one_hot_encoding = pd.get_dummies(data)\n",
    "\n",
    "#checking for missing column in hexa and fill with zeros [0...9 a...f]\n",
    "for character in string.ascii_lowercase[0:6] + string.digits[0:]:\n",
    "    if character not in one_hot_encoding.columns:\n",
    "    \tone_hot_encoding[character] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0162cbc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot data to image for checking\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor('#000000')\n",
    "plt.imshow(one_hot_encoding)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6883e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0  1  2  3  4  5  6  7  8  9  a  b  c  f  d  e\n",
      "0    0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "1    0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0\n",
      "2    1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "3    0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "4    0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "..  .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..\n",
      "251  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "252  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "253  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0\n",
      "254  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0\n",
      "255  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "\n",
      "[256 rows x 16 columns]\n",
      "<BatchDataset element_spec=TensorSpec(shape=(None, 16), dtype=tf.int64, name=None)>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 60000\n",
    "EPOCHS = 50\n",
    "\n",
    "print(one_hot_encoding)\n",
    "\n",
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(one_hot_encoding) \\\n",
    "    .shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# #change data dimension\n",
    "# train_dataset = train_dataset.transpose([None,48,64,1])\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b552b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define generator and discriminator\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    dropout = 0.4\n",
    "\n",
    "    model.add(Dense(3*4*512, activation=\"relu\", input_dim=256))\n",
    "    model.add(Reshape((3,4,512)))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(256,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(128,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(64,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation(\"relu\"))\n",
    "   \n",
    "    # Output resolution, additional upsampling\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(1,kernel_size=3,padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_discriminator(image_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    #testing image_shape\n",
    "#     image_shape=(64,48)\n",
    "    model.add(Dense(48*64*1, activation=\"relu\"))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=image_shape, \n",
    "                     padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25)) #random set input to 0 prevent overfitting\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(16, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#     model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4384877e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_112\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_104 (Dense)           (None, 6144)              1579008   \n",
      "                                                                 \n",
      " reshape_71 (Reshape)        (None, 3, 4, 512)         0         \n",
      "                                                                 \n",
      " dropout_139 (Dropout)       (None, 3, 4, 512)         0         \n",
      "                                                                 \n",
      " up_sampling2d_277 (UpSampli  (None, 6, 8, 512)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_transpose_276 (Conv2  (None, 6, 8, 256)        1179904   \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " batch_normalization_253 (Ba  (None, 6, 8, 256)        1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_276 (Activation)  (None, 6, 8, 256)        0         \n",
      "                                                                 \n",
      " up_sampling2d_278 (UpSampli  (None, 12, 16, 256)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_transpose_277 (Conv2  (None, 12, 16, 128)      295040    \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " batch_normalization_254 (Ba  (None, 12, 16, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_277 (Activation)  (None, 12, 16, 128)      0         \n",
      "                                                                 \n",
      " up_sampling2d_279 (UpSampli  (None, 24, 32, 128)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_transpose_278 (Conv2  (None, 24, 32, 64)       73792     \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " batch_normalization_255 (Ba  (None, 24, 32, 64)       256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_278 (Activation)  (None, 24, 32, 64)       0         \n",
      "                                                                 \n",
      " up_sampling2d_280 (UpSampli  (None, 48, 64, 64)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_transpose_279 (Conv2  (None, 48, 64, 1)        577       \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " activation_279 (Activation)  (None, 48, 64, 1)        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,130,113\n",
      "Trainable params: 3,129,217\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_113\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_105 (Dense)           (None, 6144)              1579008   \n",
      "                                                                 \n",
      " reshape_72 (Reshape)        (None, 3, 4, 512)         0         \n",
      "                                                                 \n",
      " dropout_140 (Dropout)       (None, 3, 4, 512)         0         \n",
      "                                                                 \n",
      " up_sampling2d_281 (UpSampli  (None, 6, 8, 512)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_transpose_280 (Conv2  (None, 6, 8, 256)        1179904   \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " batch_normalization_256 (Ba  (None, 6, 8, 256)        1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_280 (Activation)  (None, 6, 8, 256)        0         \n",
      "                                                                 \n",
      " up_sampling2d_282 (UpSampli  (None, 12, 16, 256)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_transpose_281 (Conv2  (None, 12, 16, 128)      295040    \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " batch_normalization_257 (Ba  (None, 12, 16, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_281 (Activation)  (None, 12, 16, 128)      0         \n",
      "                                                                 \n",
      " up_sampling2d_283 (UpSampli  (None, 24, 32, 128)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_transpose_282 (Conv2  (None, 24, 32, 64)       73792     \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " batch_normalization_258 (Ba  (None, 24, 32, 64)       256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_282 (Activation)  (None, 24, 32, 64)       0         \n",
      "                                                                 \n",
      " up_sampling2d_284 (UpSampli  (None, 48, 64, 64)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_transpose_283 (Conv2  (None, 48, 64, 1)        577       \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " activation_283 (Activation)  (None, 48, 64, 1)        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,130,113\n",
      "Trainable params: 3,129,217\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "IMAGE_CHANNELS = 1\n",
    "SEED_SIZE = 256\n",
    "\n",
    "generator = build_generator()\n",
    "generated_image = build_generator()\n",
    "image_shape = (64,16*3,IMAGE_CHANNELS)\n",
    "\n",
    "discriminator = build_discriminator(image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5c8d9b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "cf90b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "  seed = tf.random.normal([BATCH_SIZE, SEED_SIZE])\n",
    "\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    generated_images = generator(seed, training=True)\n",
    "\n",
    "    real_output = discriminator(images, training=True)\n",
    "    fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "    gen_loss = generator_loss(fake_output)\n",
    "    disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(\\\n",
    "        gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(\\\n",
    "        disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(\n",
    "        gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(\n",
    "        gradients_of_discriminator, \n",
    "        discriminator.trainable_variables))\n",
    "  return gen_loss,disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c58f246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  start = time.time()\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "  for image_batch in dataset:\n",
    "    t = train_step(image_batch)\n",
    "\n",
    "    epoch_elapsed = time.time()-epoch_start\n",
    "    print (f'Epoch {epoch+1},'' {hms_string(epoch_elapsed)}')\n",
    "\n",
    "  elapsed = time.time()-start\n",
    "  print (f'Training time: {hms_string(elapsed)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c9591190",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_12908/2008501276.py\", line 8, in train_step  *\n        real_output = discriminator(images, training=True)\n    File \"/home/tailoc/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/tailoc/anaconda3/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_114' (type Sequential).\n    \n    Input 0 of layer \"conv2d_80\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (64, 3072)\n    \n    Call arguments received by layer 'sequential_114' (type Sequential):\n      • inputs=tf.Tensor(shape=(64, 16), dtype=int64)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12908/912791478.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train_dataset.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_12908/1722704304.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mepoch_elapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepoch_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/__autograph_generated_filenah4ucyx.py\u001b[0m in \u001b[0;36mtf__train_step\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgen_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdisc_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mgenerated_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                     \u001b[0mreal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                     \u001b[0mfake_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mgen_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    251\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_12908/2008501276.py\", line 8, in train_step  *\n        real_output = discriminator(images, training=True)\n    File \"/home/tailoc/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/tailoc/anaconda3/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_114' (type Sequential).\n    \n    Input 0 of layer \"conv2d_80\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (64, 3072)\n    \n    Call arguments received by layer 'sequential_114' (type Sequential):\n      • inputs=tf.Tensor(shape=(64, 16), dtype=int64)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "# train_dataset.shape\n",
    "train(train_dataset, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
