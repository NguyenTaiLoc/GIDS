{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b619be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense \n",
    "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import math\n",
    "import sys\n",
    "\n",
    "#show full numpy matrix data\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "#loading CAN IDs from file\n",
    "fp = open('/media/tailoc/D/GIDS/CAN_IDS/test_data.txt', 'r')\n",
    "content = fp.readlines()\n",
    "\n",
    "data = []\n",
    "for i in range(len(content[0])):\n",
    "\tif (content[0][i] != ','):\n",
    "\t\tdata.append(content[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "324a27bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36838/747206275.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  one_hot_encoding_48bits = pd.DataFrame(dtype=np.float, columns=['0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f',\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0  1  2  3  4  5  6  7  8  9  ...  6_2  7_2  8_2  9_2  a_2  b_2  c_2  \\\n",
      "0     0  0  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "1     0  0  1  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "2     0  0  0  0  1  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "3     0  0  0  0  1  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "4     0  1  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "...  .. .. .. .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "6139  0  0  0  0  1  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "6140  0  1  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "6141  0  1  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "6142  1  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "6143  0  1  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "\n",
      "      d_2  e_2  f_2  \n",
      "0       0    0    0  \n",
      "1       0    0    0  \n",
      "2       0    0    0  \n",
      "3       0    0    0  \n",
      "4       0    0    0  \n",
      "...   ...  ...  ...  \n",
      "6139    0    0    0  \n",
      "6140    0    0    0  \n",
      "6141    0    0    0  \n",
      "6142    0    0    0  \n",
      "6143    0    0    0  \n",
      "\n",
      "[6144 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "#one hot encoding data\n",
    "one_hot_encoding = pd.get_dummies(data)\n",
    "one_hot_encoding_48bits = pd.DataFrame(dtype=np.float, columns=['0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f',\\\n",
    "                                                '0_1','1_1','2_1','3_1','4_1','5_1','6_1','7_1','8_1','9_1','a_1','b_1','c_1','d_1','e_1','f_1',\\\n",
    "                                                '0_2','1_2','2_2','3_2','4_2','5_2','6_2','7_2','8_2','9_2','a_2','b_2','c_2','d_2','e_2','f_2'\\\n",
    "                                               ])\n",
    "\n",
    "#checking for missing column in hexa and fill with zeros [0...9 a...f]\n",
    "for character in string.ascii_lowercase[0:6] + string.digits[0:]:\n",
    "    if character not in one_hot_encoding.columns:\n",
    "    \tone_hot_encoding[character] = 0\n",
    "        \n",
    "for i in range(len(one_hot_encoding)):\n",
    "    if ((i+1) % 3 == 1):\n",
    "        one_hot_encoding_48bits.loc[len(one_hot_encoding_48bits)] = 0\n",
    "        for j in range(one_hot_encoding_48bits.columns.get_loc('0'), one_hot_encoding_48bits.columns.get_loc('f')+1):\n",
    "            one_hot_encoding_48bits.loc[math.floor((i+1)/3)][j] = one_hot_encoding.loc[i][j]\n",
    "    elif ((i+1) % 3 == 2):\n",
    "        for j in range(one_hot_encoding_48bits.columns.get_loc('0_1'), one_hot_encoding_48bits.columns.get_loc('f_1')+1):\n",
    "            one_hot_encoding_48bits.loc[math.floor((i+1)/3)][j] = one_hot_encoding.loc[i][j-16]\n",
    "    else:\n",
    "        for j in range(one_hot_encoding_48bits.columns.get_loc('0_2'), one_hot_encoding_48bits.columns.get_loc('f_2')+1):\n",
    "            one_hot_encoding_48bits.loc[math.floor((i+1)/3)-1][j] = one_hot_encoding.loc[i][j-32]\n",
    "            \n",
    "# one_hot_encoding_48bits.to_csv('/home/tailoc/Downloads/GIDS_dataset/CAN_IDS/test_output_data.csv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "print(one_hot_encoding_48bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0162cbc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot data to image for checking\n",
    "figure(dpi=100) #size\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor('#000000') #color\n",
    "\n",
    "plt.imshow(one_hot_encoding)\n",
    "plt.show()\n",
    "\n",
    "# np.random.seed(101)\n",
    "# g = np.floor(np.random.random((100, 100)) + .5)\n",
    "# plt.imshow(one_hot_encoding_48bits, cmap='Greys',  interpolation='nearest')\n",
    "\n",
    "# plt.show()\n",
    "figure(dpi=100)\n",
    "plt.imshow(one_hot_encoding_48bits)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6883e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0  1  2  3  4  5  6  7  8  9  ...  6_2  7_2  8_2  9_2  a_2  b_2  c_2  \\\n",
      "0     0  0  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "1     0  0  1  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "2     0  0  0  0  1  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "3     0  0  0  0  1  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "4     0  1  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "...  .. .. .. .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "1481  0  0  0  0  1  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "1482  0  0  1  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "1483  0  0  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "1484  0  0  0  0  0  0  1  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "1485  0  0  0  0  1  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "\n",
      "      d_2  e_2  f_2  \n",
      "0       0    0    0  \n",
      "1       0    0    0  \n",
      "2       0    0    0  \n",
      "3       0    0    0  \n",
      "4       0    0    0  \n",
      "...   ...  ...  ...  \n",
      "1481    0    0    0  \n",
      "1482    0    0    0  \n",
      "1483    0    0    0  \n",
      "1484    0    0    0  \n",
      "1485    0    0    0  \n",
      "\n",
      "[1486 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 60000\n",
    "EPOCHS = 50\n",
    "\n",
    "print(one_hot_encoding_48bits)\n",
    "\n",
    "# #change data dimension\n",
    "# train_dataset = train_dataset.transpose([None,48,64,1])\n",
    "\n",
    "# print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b552b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define generator and discriminator\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    dropout = 0.4\n",
    "\n",
    "    model.add(Dense(3*4*512, activation=\"relu\", input_dim=256))\n",
    "    model.add(Reshape((3,4,512)))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(256,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(128,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(64,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation(\"relu\"))\n",
    "   \n",
    "    # Output resolution, additional upsampling\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(1,kernel_size=3,padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_discriminator(image_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    #testing image_shape\n",
    "#     image_shape=(64,48)\n",
    "\n",
    "#     model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=( 64, 48, 1), \n",
    "                     padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "#     model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25)) #random set input to 0 prevent overfitting\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "#     model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.build(image_shape)\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4384877e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 6144)              1579008   \n",
      "                                                                 \n",
      " reshape_6 (Reshape)         (None, 3, 4, 512)         0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 3, 4, 512)         0         \n",
      "                                                                 \n",
      " up_sampling2d_24 (UpSamplin  (None, 6, 8, 512)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_24 (Conv2D  (None, 6, 8, 256)        1179904   \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 6, 8, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 6, 8, 256)         0         \n",
      "                                                                 \n",
      " up_sampling2d_25 (UpSamplin  (None, 12, 16, 256)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_25 (Conv2D  (None, 12, 16, 128)      295040    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 12, 16, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_31 (Activation)  (None, 12, 16, 128)       0         \n",
      "                                                                 \n",
      " up_sampling2d_26 (UpSamplin  (None, 24, 32, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_26 (Conv2D  (None, 24, 32, 64)       73792     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 24, 32, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_32 (Activation)  (None, 24, 32, 64)        0         \n",
      "                                                                 \n",
      " up_sampling2d_27 (UpSamplin  (None, 48, 64, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_27 (Conv2D  (None, 48, 64, 1)        577       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " activation_33 (Activation)  (None, 48, 64, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,130,113\n",
      "Trainable params: 3,129,217\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 6144)              1579008   \n",
      "                                                                 \n",
      " reshape_7 (Reshape)         (None, 3, 4, 512)         0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 3, 4, 512)         0         \n",
      "                                                                 \n",
      " up_sampling2d_28 (UpSamplin  (None, 6, 8, 512)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_28 (Conv2D  (None, 6, 8, 256)        1179904   \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 6, 8, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_34 (Activation)  (None, 6, 8, 256)         0         \n",
      "                                                                 \n",
      " up_sampling2d_29 (UpSamplin  (None, 12, 16, 256)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_29 (Conv2D  (None, 12, 16, 128)      295040    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 12, 16, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_35 (Activation)  (None, 12, 16, 128)       0         \n",
      "                                                                 \n",
      " up_sampling2d_30 (UpSamplin  (None, 24, 32, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_30 (Conv2D  (None, 24, 32, 64)       73792     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 24, 32, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_36 (Activation)  (None, 24, 32, 64)        0         \n",
      "                                                                 \n",
      " up_sampling2d_31 (UpSamplin  (None, 48, 64, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_31 (Conv2D  (None, 48, 64, 1)        577       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " activation_37 (Activation)  (None, 48, 64, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,130,113\n",
      "Trainable params: 3,129,217\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 32, 24, 64)        640       \n",
      "                                                                 \n",
      " activation_38 (Activation)  (None, 32, 24, 64)        0         \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 32, 24, 64)        0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 16, 12, 32)        18464     \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 16, 12, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_39 (Activation)  (None, 16, 12, 32)        0         \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 16, 12, 32)        0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 6144)              0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 6145      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,377\n",
      "Trainable params: 25,313\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "IMAGE_CHANNELS = 1\n",
    "SEED_SIZE = 256\n",
    "\n",
    "generator = build_generator()\n",
    "generated_image = build_generator()\n",
    "image_shape = (None,64,48,IMAGE_CHANNELS)\n",
    "\n",
    "discriminator = build_discriminator(image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf90b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "  print(images)\n",
    "  seed = tf.random.normal([BATCH_SIZE, SEED_SIZE])\n",
    "\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    generated_images = generator(seed, training=True)\n",
    "\n",
    "    print(images)\n",
    "    \n",
    "    real_output = discriminator(images, training=True)\n",
    "    fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "    gen_loss = generator_loss(fake_output)\n",
    "    disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(\\\n",
    "        gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(\\\n",
    "        disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(\n",
    "        gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(\n",
    "        gradients_of_discriminator, \n",
    "        discriminator.trainable_variables))\n",
    "  return gen_loss,disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c58f246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  start = time.time()\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "  for image_batch in dataset:\n",
    "    print(image_batch.shape)\n",
    "    t = train_step(image_batch)\n",
    "\n",
    "    epoch_elapsed = time.time()-epoch_start\n",
    "    print (f'Epoch {epoch+1},'' {hms_string(epoch_elapsed)}')\n",
    "\n",
    "  elapsed = time.time()-start\n",
    "  print (f'Training time: {hms_string(elapsed)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c9591190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=TensorSpec(shape=(None, 48), dtype=tf.int64, name=None)>\n",
      "(32, 48)\n",
      "Tensor(\"images:0\", shape=(32, 48), dtype=int64)\n",
      "Tensor(\"images:0\", shape=(32, 48), dtype=int64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_36838/536646953.py\", line 11, in train_step  *\n        real_output = discriminator(images, training=True)\n    File \"/home/tailoc/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/tailoc/.local/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_11\" is incompatible with the layer: expected shape=(None, 64, 48, 1), found shape=(32, 48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 14\u001b[0m\n\u001b[1;32m      9\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(one_hot_encoding_48bits) \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mshuffle(BUFFER_SIZE)\u001b[38;5;241m.\u001b[39mbatch(BATCH_SIZE)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_dataset)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[56], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_batch \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;28mprint\u001b[39m(image_batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 9\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m   epoch_elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mepoch_start\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mhms_string(epoch_elapsed)}\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filek8mtepuk.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     13\u001b[0m generated_images \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(generator), (ag__\u001b[38;5;241m.\u001b[39mld(seed),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[1;32m     14\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(ag__\u001b[38;5;241m.\u001b[39mld(images))\n\u001b[0;32m---> 15\u001b[0m real_output \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m fake_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(discriminator), (ag__\u001b[38;5;241m.\u001b[39mld(generated_images),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[1;32m     17\u001b[0m gen_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(generator_loss), (ag__\u001b[38;5;241m.\u001b[39mld(fake_output),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/input_spec.py:295\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m--> 295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    297\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_shape(x\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_36838/536646953.py\", line 11, in train_step  *\n        real_output = discriminator(images, training=True)\n    File \"/home/tailoc/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/tailoc/.local/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_11\" is incompatible with the layer: expected shape=(None, 64, 48, 1), found shape=(32, 48)\n"
     ]
    }
   ],
   "source": [
    "# train_dataset.shape\n",
    "train_dataset = pd.DataFrame.to_numpy(one_hot_encoding_48bits)\n",
    "train_dataset = np.reshape(train_dataset,(-1,64,\n",
    "            48,1))\n",
    "\n",
    "\n",
    "# print(train_dataset.shape)\n",
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(one_hot_encoding_48bits) \\\n",
    "    .shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "print(train_dataset)\n",
    "\n",
    "train(train_dataset, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
